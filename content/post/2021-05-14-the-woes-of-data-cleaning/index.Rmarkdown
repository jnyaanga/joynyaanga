---
title: The woes of data cleaning
author: Joy Nyaanga
date: '2021-05-14'
slug: data-cleaning
categories:
  - research
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2021-05-14T18:01:29-05:00'
featured: no
image:
  caption: '[Photo by Oskar Yildiz on Unsplash](https://unsplash.com/photos/cOkpTiJMGzA?utm_source=unsplash&utm_medium=referral&utm_content=creditShareLink)'
  focal_point: ''
  preview_only: no
projects: [growth-and-dev]
---
If you've ever handled data you've probably faced the ever daunting first step of cleaning and tidying said data. As a matter of fact, it's been mentioned that **80%** of data analysis is spent solely on the process of cleaning and preparing the data [[1]](https://onlinelibrary.wiley.com/doi/book/10.1002/0471448354). And the hardest part by far is knowing where to begin.  

I have the fortune of working with high-throughput data but this necessitates a data cleaning workflow. Following the acquisition of my growth data I was left with the challenge of discerning what animal size measurements corresponded to true animals vs debri (clumped food, shed cuticles, animal carcass, etc). It's easy to see from images that debri exists, but its not so easy to make this distinction in the flow cytometer data.  

For example, after 65 hours we don't expect animals to suddenly get smaller... yet we see the emergence of a density having a small average size. 

```{r message=FALSE, warning=FALSE, include=FALSE}
library(tidyverse); library(cowplot); library(mclust)
rawdata <- readr::read_csv("data/raw.csv") %>%
  dplyr::filter(!replicate %in% c("no food", "no worms")) %>%
  dplyr::mutate(TOF = 1.67110915*TOF + 100.31575759,
                norm.EXT = 15.68506057*norm.EXT -1.04595184,
                volume = (pi/4)*TOF*(norm.EXT)^2,
                hour = as.numeric(timepoint),
                length = TOF,
                width = norm.EXT)

pruneddata <- readr::read_csv("data/pruned.csv") %>%
  dplyr::mutate(TOF = 1.67110915*TOF + 100.31575759,
                norm.EXT = 15.68506057*norm.EXT -1.04595184,
                volume = (pi/4)*TOF*(norm.EXT)^2,
                hour = as.numeric(timepoint),
                length = TOF,
                width = norm.EXT)
```


```{r echo=FALSE, fig.height=3.5, fig.width=7}
rawdata %>%
  ggplot() +
  aes(x = hour, y = TOF) +
  geom_jitter(size = 0.2, width = 0.2, alpha = 0.02)+
  scale_x_continuous(breaks = seq(0, 70, 5)) +
  labs(x="Time (hours)", y = expression(paste("Length (", mu,"m)"))) +
  theme_cowplot(font_size = 14, rel_small = 12/14)
```

Looking at images its obvious that these are actually the next generation of newly hatched animals! 

<img src='images/L1.jpg' alt='mixed stage animals' width='700' >
  
But how do we selectively remove these from our data without also removing the data we **want** to keep? I mulled over this question for a while. If I were to simply remove all small objects, I would lose information from the earlier hours of development

```{r}
filtered <- rawdata %>%
  dplyr::filter(!length < 350)
```
```{r echo=FALSE, fig.height=3.5, fig.width=7}
ggplot(filtered) +
  aes(x = hour, y = TOF) +
  geom_jitter(size = 0.2, width = 0.2, alpha = 0.02)+
  scale_x_continuous(breaks = seq(0, 70, 5)) +
  labs(x="Time (hours)", y = expression(paste("Length (", mu,"m)"))) +
  theme_cowplot(font_size = 14, rel_small = 12/14)
```

Ideally I would like a way to filter my data on an hour by hour manner without the bias of manually selecting a size threshold. For this I settled on using gaussian finite mixture modeling as implemented in the *mclust* R package. Specifically, the *mclust* package fits a mixture model to data and selects the optimal number of clusters using an expectation-maximization algorithm (EM) and Bayes Information Criteria (BIC) -- **basically it can classify unstructured data into meaningful groups**.

```{r}
H70 <- rawdata %>%
  dplyr::filter(hour == 70) %>%
  dplyr::select(length) %>%
  mclust:: Mclust(., 2)

plot(H70, what = c("density"))
```
```{r echo = FALSE}
summary(H70, parameters = TRUE)
```

Alright pretty cool - from the density plot its clear that there are two densities and running mclust shows us how the data was grouped. The smaller cluster of newly hatched animals has an average length of 247.8um and makes up 65% of the data at hour 70. 

This would work but to increase the performance of the model-based clustering, I decided to use log transform length and width as inputs.

```{r}
H70 <- rawdata %>%
  dplyr::filter(hour == 70) %>%
  dplyr::mutate(log.length = log(length),
                log.width = log(width)) %>%
  dplyr::select(log.length, log.width) %>%
  mclust:: Mclust(., 2)

plot(H70, what = c("classification"))
```
There we go! This way it is very clear how the two groups are being classified. Now if we do this across all the hours we can manually select the clusters most likely containing true animals while removing those not.

```{r echo=FALSE, message=FALSE, warning=FALSE}
removed <- dplyr::anti_join(rawdata, pruneddata) %>%
  dplyr::mutate(`Removed ?` = "Yes")

kept <- pruneddata %>%
  dplyr::mutate(`Removed ?` = "No")

total <- dplyr::full_join(removed, kept)

total %>%
  dplyr::filter(replicate == "R02" & timepoint %in% c("01","05",10,15,20,25,30,35,40,45,50,55,60,65,70)) %>%
  ggplot() +
  aes(x = log(TOF), y = log(EXT), color = `Removed ?`) +
  scale_color_manual(values = c("#BC3C29FF", "#0072B5FF")) +
  geom_jitter(size = 0.2, alpha = 0.5) +
  theme_cowplot(font_size = 10, rel_small = 8/10) +
  facet_wrap(~timepoint, nrow = 3) + panel_border()
```

This processing removes non-animal objects such as bacterial clumps, shed cuticles, and next generation larval animals.

```{r}
total %>%
  ggplot() +
  aes(x = hour, y = TOF, color = `Removed ?`) +
  scale_color_manual(values = c("#BC3C29FF", "#0072B5FF")) +
  geom_jitter(size = 0.3, width = 0.2, alpha = 0.03)+
  scale_x_continuous(breaks = seq(0, 70, 5)) +
  labs(x="Time (hours)", y = expression(paste("Length (", mu,"m)"))) +
  theme_cowplot(font_size = 14, rel_small = 12/14) +  
  guides(color = F)
```

And just like that our data is clean! Now for the fun part -- data analysis.

